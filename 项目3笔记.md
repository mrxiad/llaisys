# LLAISYS 项目3笔记（AI Chatbot）

完成时间：2026-02-19

## 1. 目标

实现 `Project #3` 要求的三部分：

- 随机采样（支持 `temperature`、`top_k`、`top_p`）
- 聊天服务器（OpenAI Chat Completions 风格）
- 交互式聊天 UI（命令行）

## 2. 主要改动

### 2.1 新增采样算子

新增文件：

- `src/ops/sample/op.hpp`
- `src/ops/sample/op.cpp`

新增接口：

- C++: `llaisys::ops::sample(sampled_idx, logits, temperature, top_k, top_p)`
- C API: `llaisysSample(...)`
- Python: `llaisys.Ops.sample(...)`

实现说明：

- 输入 `logits` 为 1D 张量（`f32/f16/bf16`），输出 `sampled_idx` 为 shape `[1]` 的 `i64`
- 支持：
  - `temperature` 缩放
  - `top_k` 截断（`top_k<=0` 表示不启用 top-k）
  - `top_p` 截断（nucleus sampling）
- 使用 `std::discrete_distribution` 做随机采样

### 2.2 Qwen2 推理接口扩展

修改文件：

- `include/llaisys/models/qwen2.h`
- `src/llaisys/qwen2.cc`
- `python/llaisys/libllaisys/models_qwen2.py`
- `python/llaisys/models/qwen2.py`

新增接口：

- `llaisysQwen2ModelInferEx(model, token_ids, ntoken, top_k, top_p, temperature)`

兼容策略：

- 保留原有 `llaisysQwen2ModelInfer(...)`（greedy），内部复用 `InferEx(top_k=1, top_p=1, temperature=1)`

行为变化：

- `Qwen2.generate(...)` 现在会真正使用采样参数
- 新增 `Qwen2.stream_generate(...)`，用于流式逐 token 输出

### 2.3 聊天服务器与 CLI

新增文件：

- `python/llaisys/chat/server.py`
- `python/llaisys/chat/cli.py`
- `python/llaisys/chat/__init__.py`

服务端能力：

- 端点：`POST /v1/chat/completions`
- 请求字段兼容：
  - `model`
  - `messages`
  - `stream`
  - `max_tokens`
  - `top_k`
  - `top_p`
  - `temperature`
- 返回：
  - 非流式：`chat.completion`
  - 流式：SSE `chat.completion.chunk` + `data: [DONE]`
- 单用户串行处理：内部全局锁确保同一时刻只处理一个请求

CLI 能力：

- 持续多轮对话
- 支持 `/reset` 清空会话、`/exit` 退出
- 支持流式和非流式两种模式

## 3. 运行方式

### 3.1 启动服务端

```bash
export PYTHONPATH=python
./.venv310/bin/python -m llaisys.chat.server \
  --model /path/to/model \
  --host 127.0.0.1 \
  --port 8000 \
  --device cpu
```

### 3.2 启动 CLI

```bash
export PYTHONPATH=python
./.venv310/bin/python -m llaisys.chat.cli \
  --base-url http://127.0.0.1:8000 \
  --model qwen2 \
  --max-tokens 256 \
  --top-k 50 \
  --top-p 0.8 \
  --temperature 0.8
```

如需非流式：

```bash
./.venv310/bin/python -m llaisys.chat.cli --no-stream
```

## 4. 构建与验证

执行：

```bash
xmake -j 8
xmake install -o .
python3 -m py_compile \
  python/llaisys/chat/server.py \
  python/llaisys/chat/cli.py \
  python/llaisys/models/qwen2.py
```

结果：

- C++ 构建通过
- Python 语法检查通过
- 动态库符号已确认包含：
  - `llaisysSample`
  - `llaisysQwen2ModelInferEx`

CPU smoke 测试（2026-02-19）：

- `sample` 算子：`top_k=1` 与 `torch.argmax` 一致，随机采样路径可运行
- `Qwen2.generate/stream_generate`：本地 DeepSeek 模型 CPU 路径通过
- `chat server`：
  - 非流式 `POST /v1/chat/completions` 通过
  - 流式 SSE `POST /v1/chat/completions` 通过
- `chat CLI`：
  - CLI -> server 一轮对话（`hello`）后退出通过

## 5. 完成状态

- [x] 随机采样算子（temperature/top_k/top_p）
- [x] OpenAI 风格聊天服务端（含流式）
- [x] 命令行聊天 UI
