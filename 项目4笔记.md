# LLAISYS 项目4笔记（多用户推理服务）

完成时间：2026-02-19

## 1. 目标

按 `Project #4` 要求实现：

- 多用户并发接入（请求池/队列）
- 后台循环线程处理请求
- 连续批处理（iteration-level continuous batching）
- 流式响应能力

## 2. 实现内容

### 2.1 新增多用户服务端

新增文件：

- `python/llaisys/chat/multi_server.py`

核心结构：

- `ThreadingHTTPServer`：支持并发 HTTP 连接
- `MultiUserScheduler`：
  - `incoming` 请求队列
  - `pending` 活跃请求池
  - 独立 worker 线程循环调度
- `PendingRequest`：
  - 保存每个请求的 token 状态、完成事件、流式 chunk 队列

### 2.2 连续批处理调度

实现方式：

- 每轮从 `pending` 中取最多 `batch_size` 个请求组成“批”
- 每个请求在该轮只生成 1 个 token（一次 round inference）
- 未完成请求放回 `pending`，下一轮继续
- 已完成请求：
  - 非流式：写入最终 JSON，唤醒等待
  - 流式：发 finish chunk，再发送 `[DONE]`

### 2.3 前缀缓存池（Prefix Matching）

实现：

- `PrefixCachePool`
- 对确定性解码（`top_k=1 && top_p=1 && temperature=1`）缓存 `prefix -> next_token`
- 后续相同前缀请求直接复用 token，减少重复计算

## 3. 接口与启动

### 3.1 启动多用户服务

```bash
export PYTHONPATH=python
./.venv310/bin/python -m llaisys.chat.multi_server \
  --model /path/to/model \
  --host 127.0.0.1 \
  --port 8001 \
  --device cpu \
  --batch-size 4
```

端点：

- `POST /v1/chat/completions`
- 兼容 `stream=false/true` 两种模式

## 4. 验证结果（CPU）

已执行：

- 语法检查：
  - `python3 -m py_compile python/llaisys/chat/multi_server.py`
- 并发非流式 smoke：
  - 同时发送 2 个请求，均返回 `chat.completion`
  - 结果：`PASS`
- 流式 smoke：
  - 单请求 SSE 收到 `chat.completion.chunk` 与 `[DONE]`
  - 结果：`PASS`

## 5. 当前边界

- 当前 Qwen2 后端仍以单序列推理为主，因此“每轮 1 token”实现中，单请求推理开销较高。
- 目前是服务层的连续调度与队列批处理；真正张量级 batched matmul 仍需后端批推理算子配合。
- 前缀缓存池为服务层 token 前缀缓存，不是底层 KV-cache 直接共享。

## 6. 完成状态

- [x] 多用户并发接入（队列 + 线程服务）
- [x] 连续批处理调度（iteration-level）
- [x] 流式与非流式返回
- [x] Prefix matching 缓存池（确定性路径）
