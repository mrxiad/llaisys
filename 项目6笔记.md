# LLAISYS 项目6笔记（支持新模型类型）

完成时间：2026-02-19

## 1. 目标

按 `Project #6` 要求，为 LLAISYS 增加除作业模型（Qwen2）之外的另一种模型类型支持。

本次新增：`Llama` 模型类型（CPU 路径）。

## 2. 主要实现

### 2.1 新增 Llama 模型封装

新增文件：

- `python/llaisys/models/llama.py`

实现：

- 新增 `Llama` 类，复用当前后端兼容的 decoder-only 推理路径。

### 2.2 新增模型自动识别与工厂加载

新增文件：

- `python/llaisys/models/factory.py`

新增接口：

- `detect_model_type(model_path)`：
  - 读取 `config.json`
  - 支持根据 `model_type` / `architectures` 识别 `llama` 与 `qwen2`
- `load_model(model_path, device)`：
  - 自动构建 `Llama` 或 `Qwen2`

### 2.3 对上层服务做自动模型接入

修改文件：

- `python/llaisys/models/__init__.py`
- `python/llaisys/chat/server.py`
- `python/llaisys/chat/multi_server.py`
- `python/llaisys/distributed/mpi_tp_qwen2.py`

改动点：

- 统一由 `llaisys.models.load_model(...)` 创建模型实例，不再硬编码 `Qwen2(...)`

## 3. 验证结果

### 3.1 语法检查

执行：

```bash
python3 -m py_compile \
  python/llaisys/models/llama.py \
  python/llaisys/models/factory.py \
  python/llaisys/models/__init__.py \
  python/llaisys/chat/server.py \
  python/llaisys/chat/multi_server.py \
  python/llaisys/distributed/mpi_tp_qwen2.py
```

结果：通过。

### 3.2 Qwen2 回归测试

执行：

```bash
./.venv310/bin/python test/test_infer.py \
  --model /home/mrxiad/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562 \
  --test --max_steps 1 --prompt "Hi"
```

结果：

- `Test passed!`
- 与 PyTorch 对齐。

### 3.3 Llama 新模型类型烟测（零额外模型下载）

方法：

- 基于已存在本地模型目录创建临时目录 `/tmp/llaisys_project6_llama_smoke`
- 仅改写 `config.json` 的 `model_type=llama`、`architectures=[LlamaForCausalLM]`
- 其余大文件使用符号链接（不复制权重）

结果：

- `detect_model_type` 识别成功：`qwen2 -> llama`
- `load_model` 返回类：`Llama`
- 生成 1 token 输出可用（`Alright`）

## 4. 磁盘占用说明

- 本次新增源码与笔记均为小文件（KB 级）
- Llama 烟测使用符号链接复用已有权重，未下载新模型，未复制大权重文件

## 5. 完成状态

- [x] 新增模型类型：`Llama`
- [x] 自动模型识别与工厂加载
- [x] 聊天服务/多用户服务/分布式入口支持自动模型
- [x] 回归与新模型烟测通过
